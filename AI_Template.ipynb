{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI Template.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CrqbWcYQEmef",
        "ehimIOolvhyx",
        "9iE2f3VsvpbV",
        "ltsE6aOgKlgW",
        "lrM8kVYAGeHt",
        "Wrwt-4uyF5Jj",
        "tj-Bn_gPGwWx",
        "Jicc8HQyHNa9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbkvu5pk49uq"
      },
      "source": [
        "# General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdUZdLO63ssO"
      },
      "source": [
        "!nvidia-smi # Check what gpu you are connected with"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF0F0HFB5GPm"
      },
      "source": [
        "# Installments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqSm_H5W__QQ"
      },
      "source": [
        "## General"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j770Kp1XAA7c"
      },
      "source": [
        "# Import generally needed modules\n",
        "import torch # Imports torch\n",
        "\n",
        "from math import sqrt # square root\n",
        "import numpy as np # Handels conversion from image to tensor\n",
        "import random\n",
        "\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from PIL import Image # Handels images\n",
        "\n",
        "import os, sys # Handels interacting with the operating system"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc0FrMVA5J8I"
      },
      "source": [
        "## Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbflQeq346Yt"
      },
      "source": [
        "# Pip install necessary modules for working with transformers at high and low level\n",
        "!pip install datasets\n",
        "!pip install transformers[sentencepiece]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63VtICaT5Sdy"
      },
      "source": [
        "# Import specific classes from transformers module\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "# Imports for handeling datasets\n",
        "from datasets import load_dataset, dataset_dict, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWBb_a0F5iem"
      },
      "source": [
        "# Function for computing metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "def ComputeMetrics(prediction):\n",
        "    labels = prediction.label_ids\n",
        "    predictions = prediction.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    return { 'accuracy': accuracy }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrqbWcYQEmef"
      },
      "source": [
        "## Fast AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTFrcQWPEo0u"
      },
      "source": [
        "!pip install fastai --upgrade -q\n",
        "!pip install nbdev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E92wh5wFPJZ"
      },
      "source": [
        "from fastai.vision.all import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95HaER8P5B37"
      },
      "source": [
        "# Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTLHgl_PKyXr"
      },
      "source": [
        "# Mounts your drive\n",
        "def MountDrive() -> str:\n",
        "    from google.colab import drive\n",
        "    drive_directory:str = \"/content/drive\"\n",
        "    drive.mount(drive_directory)\n",
        "    return drive_directory\n",
        "\n",
        "# Functions to help with using your drive\n",
        "def CopyFolder(copy_from:str, copy_to:str):\n",
        "    %cp -av copy_from copy_to"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aduIfcQUA_e6"
      },
      "source": [
        "# Train Transformer using Trainer API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yE98SOd5Dnqe"
      },
      "source": [
        "# Model\n",
        "model_name:str = \"bert-base-uncased\" # \"KB/bert-base-swedish-cased\"\n",
        "max_lenght:int = 128\n",
        "batch_size:int = 32\n",
        "use_floating_point_16:bool = False\n",
        "training_epochs:float = 1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8nya1CJCKUa"
      },
      "source": [
        "# Mount drive\n",
        "MountDrive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehimIOolvhyx"
      },
      "source": [
        "## Get dataset from huggingface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cTUftbW4ChN"
      },
      "source": [
        "# Get and show dataset from huggingface\n",
        "dataset_name:str = \"ag_news\"\n",
        "label_list:list = [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"] # Should be able to extract from data rather than the website lamao\n",
        "raw_datasets = load_dataset(dataset_name)\n",
        "print(raw_datasets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "366tZsmaNkfm"
      },
      "source": [
        "# Tokenize and add structure to dataset\n",
        "def TokenizeFunction(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=True, max_length=max_lenght)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_dataset_train = raw_datasets[\"train\"].map(TokenizeFunction, batched=True)\n",
        "tokenized_dataset_test = raw_datasets[\"test\"].map(TokenizeFunction, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "print(data_collator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iE2f3VsvpbV"
      },
      "source": [
        "## Create dataset from file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lyq4tPNMY7B"
      },
      "source": [
        "def CreateDatasetFromJSONFile(file_path:str, label_list:list, seperation_key:str = \"[SEP]\", beggining_key:str = \"[CLS]\") -> dict:\n",
        "    import json\n",
        "\n",
        "    with open(file_path, \"r\") as json_file:\n",
        "        json_list:list = list(json_file)\n",
        "\n",
        "    dataset:dict = {\n",
        "        \"text\":[],\n",
        "        \"label\":[]\n",
        "    }\n",
        "\n",
        "    for json_str in json_list[:]:\n",
        "        result:dict = json.loads(json_str)\n",
        "\n",
        "        #result_prittie_print = json.dumps(result, sort_keys=True, indent=4)\n",
        "        #filtered_result:dict = {key: result[key] for key in result.keys() & {\"gold_label\", \"sentence1\", \"sentence2\"}}\n",
        "\n",
        "        sub_scentence_1:str = result[\"sentence1\"]\n",
        "        sub_scentence_2:str = result[\"sentence2\"]\n",
        "        full_scentence:str = f\"{beggining_key} {sub_scentence_1} {seperation_key} {sub_scentence_2} {seperation_key}\"\n",
        "        full_scentence_flipped:str = f\"{beggining_key} {sub_scentence_2} {seperation_key} {sub_scentence_1} {seperation_key}\"\n",
        "\n",
        "        label:str = result[\"gold_label\"]\n",
        "        label_index:int = label_list.index(label)\n",
        "\n",
        "        dataset[\"text\"].append(full_scentence)\n",
        "        dataset[\"label\"].append(label_index)\n",
        "\n",
        "        dataset[\"text\"].append(full_scentence_flipped)\n",
        "        dataset[\"label\"].append(label_index)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SukfLfuYQURs"
      },
      "source": [
        "dataset_name:str = \"SNLI\"\n",
        "label_list:list = [\"entailment\", \"contradiction\", \"neutral\", \"-\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmYn4VlJvuVh"
      },
      "source": [
        "Get dataset from json file\n",
        "dataset_path = Download(\"https://nlp.stanford.edu/projects/snli/snli_1.0.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdFMEHB5vG5G"
      },
      "source": [
        "dev_data:dict = CreateDatasetFromJSONFile(\"/content/snli_1.0/snli_1.0_dev.jsonl\", label_list)\n",
        "dev_dataset = Dataset.from_dict(dev_data)\n",
        "\n",
        "test_data:dict = CreateDatasetFromJSONFile(\"/content/snli_1.0/snli_1.0_test.jsonl\", label_list)\n",
        "test_dataset = Dataset.from_dict(test_data)\n",
        "\n",
        "train_data:dict = CreateDatasetFromJSONFile(\"/content/snli_1.0/snli_1.0_train.jsonl\", label_list)\n",
        "train_dataset = Dataset.from_dict(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXQrbBSWNZMz"
      },
      "source": [
        "# Tokenize and add structure to dataset\n",
        "def TokenizeFunction(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=True, max_length=max_lenght)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_dataset_train = train_dataset.map(TokenizeFunction, batched=True)\n",
        "tokenized_dataset_test = test_dataset.map(TokenizeFunction, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "print(data_collator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltsE6aOgKlgW"
      },
      "source": [
        "## Create dataset from scraping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9CGhCg0Koyr"
      },
      "source": [
        "def CreateDatasetFromScraping(page_url:str, label_list:list, seperation_key:str = \"[SEP]\", beggining_key:str = \"[CLS]\", start_itteration:int = 0, end_itteration:int = 1000): -> dict\n",
        "    dataset:dict = {\n",
        "        \"text\":[],\n",
        "        \"label\":[]\n",
        "    }\n",
        "\n",
        "    print(\"page: \", end=\"\")\n",
        "    itteration:int = start_itteration\n",
        "    while True:\n",
        "        page = requests.get(f\"{page_url}={itteration}\")\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        all_articles_in_page = soup.find_all(class_=\"inner_article\")\n",
        "\n",
        "        if len(all_articles_in_page) == 0 or end_itteration == itteration:\n",
        "            break\n",
        "\n",
        "        print(f\"{itteration}, \", end=\"\")\n",
        "        for article in all_articles_in_page:\n",
        "            review_title:str = article.find(class_=\"review_title\").contents[0][1:-1]\n",
        "            review_score:int = len(article.find_all(class_=\"mz_star_on\"))-1\n",
        "            dataset[\"text\"].append(f\"{beggining_key} {review_title} {seperation_key}\")\n",
        "            dataset[\"label\"].append(review_score)\n",
        "        itteration+=1\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thqNjeMCP_6a"
      },
      "source": [
        "dataset_name:str = \"moviezine\"\n",
        "label_list:list = [\"1\", \"2\", \"3\", \"4\", \"5\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ7rsxunMoZj"
      },
      "source": [
        "raw_dataset_train = CreateDatasetFromScraping(\"https://www.moviezine.se/recensioner?page\", label_list, end_itteration=290)\n",
        "raw_dataset_test = CreateDatasetFromScraping(\"https://www.moviezine.se/recensioner?page\", label_list, start_itteration=290)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thsk2SodNJgk"
      },
      "source": [
        "dataset_train = Dataset.from_dict(raw_dataset_train)\n",
        "dataset_test = Dataset.from_dict(raw_dataset_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skw8fhN4plUO"
      },
      "source": [
        "# Tokenize and add structure to dataset\n",
        "def TokenizeFunction(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=True, max_length=max_lenght)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenized_datasets = raw_datasets.map(TokenizeFunction, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "print(data_collator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhTKwI2uv5r4"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kW2JatzNA9VU"
      },
      "source": [
        "# Create training arguments\n",
        "training_args = TrainingArguments(f\"/content/drive/My Drive/Colab Notebooks/Models/{model_name}/{dataset_name}\" if os.path.exists(\"/content/drive/\") else \"/content/cached-trainer\", evaluation_strategy=\"epoch\")\n",
        "training_args.per_device_train_batch_size = batch_size\n",
        "training_args.per_device_eval_batch_size = batch_size\n",
        "training_args.fp16 = use_floating_point_16                # Tesla K80 CANT UTILIZE AAH\n",
        "training_args.fp16_full_eval = use_floating_point_16    # Tesla K80 CANT UTILIZE AAH\n",
        "training_args.num_train_epochs = training_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4O0vkdTBX_n"
      },
      "source": [
        "# Load in model\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_list)).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhfv3sQ64FmA"
      },
      "source": [
        "# Create a trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_test,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=ComputeMetrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jfgJgmov8Ts"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjbzyDq4Gn4"
      },
      "source": [
        "# Train\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN8eGbjRJ3eG"
      },
      "source": [
        "# Mount drive and copy cached files to drive\n",
        "if os.path.exists(\"/content/cached-trainer/\"):\n",
        "    MountDrive()\n",
        "    CopyFolder(\"/content/cached-trainer/\", f\"/content/drive/My Drive/Colab Notebooks/Models/{model_name}/{dataset_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vFK7c83FVRi"
      },
      "source": [
        "# Creating Fast Ai Backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k65ayIPAFauk"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrM8kVYAGeHt"
      },
      "source": [
        "### Creating data handler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofaWx8VhFcj6"
      },
      "source": [
        "class Path:\n",
        "  ''' Handels accessing files from directories '''\n",
        "\n",
        "  @staticmethod\n",
        "  def GetPaths(directory: str, file_extension: str = \"\") -> tuple:\n",
        "    ''' Returns all paths from given directory that ends with file_extension '''\n",
        "    paths_list: list = []\n",
        "    for (directory_path, _, file_names) in os.walk(directory):\n",
        "      for file_name in file_names:\n",
        "        if file_name.endswith(file_extension):\n",
        "          paths_list.append(f\"{directory_path}/{file_name}\")\n",
        "\n",
        "    paths: tuple = tuple(paths_list)\n",
        "    return paths\n",
        "\n",
        "  @staticmethod\n",
        "  def GetFilesFromPaths(paths: tuple, FileReaderFunction) -> tuple:\n",
        "    ''' Reads and returns all files from given paths object '''\n",
        "    files_list: list = []\n",
        "\n",
        "    for directory_path in paths:\n",
        "      file = FileReaderFunction(directory_path)\n",
        "      files_list.append(file)\n",
        "\n",
        "    files: tuple = tuple(files_list)\n",
        "    return files\n",
        "\n",
        "  @staticmethod\n",
        "  def GetFilesFromPath(directory: str, FileReaderFunction, data_type: str = \"\") -> tuple:\n",
        "    ''' Reads and returns all files from given directory which file names ends with data_type '''\n",
        "    paths: tuple = GetPaths(directory, data_type)\n",
        "    dataset: tuple = GetFilesFromPaths(paths, FileReaderFunction)\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp6DKG5-Femu"
      },
      "source": [
        "class Annotations:\n",
        "  ''' Handels accessing files from directories '''\n",
        "\n",
        "  @staticmethod\n",
        "  def GetAnnotationsByDirectory(directory: str) -> dict:\n",
        "    ''' Returns a mapped annotations to indices dictionary by hierarchy of given directory '''\n",
        "    annotations: dict = {}\n",
        "    value: int = 0\n",
        "\n",
        "    for (_, parrent_names, _) in os.walk(directory):\n",
        "      for parrent_name in parrent_names:\n",
        "        annotations[parrent_name] = value\n",
        "        value += 1\n",
        "\n",
        "    return annotations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7RE8eMbFg_r"
      },
      "source": [
        "class FileReader:\n",
        "  ''' Handels reading of files '''\n",
        "  \n",
        "  @staticmethod\n",
        "  def ReadImageFile(path: str, annotation_to_vocab: dict) -> tuple:\n",
        "    ''' Reads image files '''\n",
        "    image = Image.open(path)\n",
        "    image_resized = image.resize((64,64))\n",
        "    rgb_image = image_resized.convert('RGB')\n",
        "    image_to_tensor: torch.tensor = torch.from_numpy(np.float16(rgb_image) / 256)\n",
        "\n",
        "    annotation: str = path.split(\"/\")[-2]\n",
        "    annotation_vocab: int = annotation_to_vocab[annotation]\n",
        "    annotation_tensor: torch.tensor = torch.tensor(annotation_vocab)\n",
        "    \n",
        "    return (image_to_tensor, annotation_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcw4hcz7FiJv"
      },
      "source": [
        "class Dataset:\n",
        "  ''' Class that handels your dataset '''\n",
        "\n",
        "  def __init__(self, data_directory_paths: tuple, annotation_vocab: dict, FileReaderFunction):\n",
        "    ''' Stores data directory paths and the prefered file reader for data access '''\n",
        "    self.data_directory_paths: tuple = data_directory_paths;\n",
        "    self.FileReaderFunction = FileReaderFunction;\n",
        "    self.annotation_vocab: dict = annotation_vocab\n",
        "\n",
        "  def Length(self) -> int:\n",
        "    ''' Returns lenght of dataset '''\n",
        "    return len(self.data_directory_paths)\n",
        "\n",
        "  def GetItem(self, item_index: int) -> tuple:\n",
        "    ''' Returns item at given index '''\n",
        "    item_directory_path: str = self.data_directory_paths[item_index]\n",
        "    return self.FileReaderFunction(item_directory_path, self.annotation_vocab)\n",
        "\n",
        "  def GetBatch(self, item_indices: list, index_shift: int = 0) -> tuple:\n",
        "    ''' Returns a stacked tensor of each item in item_indices from dataset '''\n",
        "    data_tensors: list = []\n",
        "    annotations: list = []\n",
        "\n",
        "    for item_index in item_indices:\n",
        "      item: tuple = self.GetItem(item_index + index_shift)\n",
        "      data_tensors.append(item[0])\n",
        "      annotations.append(item[1])\n",
        "\n",
        "    data_collective: torch.tensor = torch.stack(data_tensors)\n",
        "    annotations_collective: torch.tensor = torch.stack(annotations)\n",
        "\n",
        "    return (data_collective, annotations_collective)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6WjN6bYFjeW"
      },
      "source": [
        "class DataLoader:\n",
        "  ''' Class working for Dataset '''\n",
        "  def __init__(self, dataset: Dataset, batch_size: int = 128, shuffle: bool = False, thread_count: int = 1):\n",
        "    self.dataset: Dataset = dataset\n",
        "    self.batch_size: int = batch_size\n",
        "    self.chunk_size: int = (self.dataset.Length() - 1) // self.batch_size + 1\n",
        "    self.shuffle: bool = shuffle\n",
        "    self.thread_count: int = thread_count\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.chunk_size\n",
        "\n",
        "  def __iter__(self) -> iter:\n",
        "    ''' Divides dataset into chunks the size of batch_size '''\n",
        "    all_indices: list = [*range(self.dataset.Length())]\n",
        "    chunks: list = []\n",
        "\n",
        "    if self.shuffle:\n",
        "      random.shuffle(all_indices)\n",
        "\n",
        "    for chunk_index in range(self.chunk_size):\n",
        "      range_from: int = chunk_index * self.batch_size\n",
        "      range_to: int = (chunk_index + 1) * self.batch_size\n",
        "\n",
        "      chunks.append(all_indices[range_from:range_to])\n",
        "    \n",
        "    data_chunks: list = []\n",
        "    for chunk in chunks:\n",
        "      data_chunks.append(self.dataset.GetBatch(chunk))\n",
        "\n",
        "    # return iter(data_chunks)\n",
        "    with concurrent.futures.ProcessPoolExecutor(self.thread_count) as thread:\n",
        "      yield from thread.map(self.dataset.GetBatch, chunks)\n",
        "\n",
        "class DataLoaders:\n",
        "    def __init__(self, *data_loaders: DataLoader):\n",
        "      self.train, self.valid = data_loaders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Npi8XjFwbv"
      },
      "source": [
        "def Download(url: str, target_directory: str = \"/content/\") -> str:\n",
        "  ''' Downloads and untars url content to target directory folder and returns path to content '''\n",
        "  !mkdir $target_directory\n",
        "\n",
        "  !wget $url\n",
        "  file_name: str = url.split(\"/\")[-1]\n",
        "  file_name_no_extension: str = file_name.split(\".\")[0]\n",
        "  file_tar_type: str = file_name.split(\".\")[-1]\n",
        "\n",
        "  downloaded_file_directory: str = \"/content/\" + file_name\n",
        "  if (file_tar_type == \"zip\"):\n",
        "    !unzip $downloaded_file_directory\n",
        "  elif (file_tar_type == \"tgz\"):\n",
        "    !tar -xvzf $downloaded_file_directory -C $target_directory\n",
        "  else:\n",
        "    print(\"tar prefix not handleable\")\n",
        "    raise ValueError;\n",
        "  !rm $downloaded_file_directory\n",
        "  \n",
        "  return target_directory + file_name_no_extension "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slinu9ThFk6s"
      },
      "source": [
        "# This cell is not mine so i do not understand it fully :'(\n",
        "def to_device(b, device=None, non_blocking=False):\n",
        "  \"Recursively put `b` on `device`.\"\n",
        "  if defaults.use_cuda==False: device='cpu'\n",
        "  elif device is None: device=default_device()\n",
        "  def _inner(o):\n",
        "      if isinstance(o,Tensor): return o.to(device, non_blocking=non_blocking)\n",
        "      return o\n",
        "  return apply(_inner, b)\n",
        "\n",
        "class Normalize:\n",
        "  def __init__(self, stats): self.stats=stats\n",
        "  def __call__(self, x):\n",
        "    if x.device != self.stats[0].device:\n",
        "      self.stats = to_device(self.stats, x.device)\n",
        "    return (x-self.stats[0])/self.stats[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrwt-4uyF5Jj"
      },
      "source": [
        "### Using created data handler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS2v0Z64F9_D"
      },
      "source": [
        "# Download dataset\n",
        "dataset_path = Download(\"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0pyUWscGBy7"
      },
      "source": [
        "# Serilize dataset into a dataloader\n",
        "paths: tuple = Path.GetPaths(dataset_path, \".jpg\")\n",
        "annotation_vocab: dict = Annotations.GetAnnotationsByDirectory(dataset_path)\n",
        "dataset: Dataset = Dataset(paths, annotation_vocab, FileReader.ReadImageFile)\n",
        "batch: tuple = dataset.GetBatch([1, 2])\n",
        "\n",
        "data_loader: DataLoader = DataLoader(dataset, 128, True)\n",
        "\n",
        "data_loaders: DataLoaders = DataLoaders(data_loader, data_loader)\n",
        "\n",
        "batches: tuple = data_loader.__iter__()\n",
        "\n",
        "# idk = data_loader.Batches()\n",
        "\n",
        "# print(idk[0][0].shape)\n",
        "# print(idk[0][1])\n",
        "\n",
        "# print(dataset.GetItem(0)[0].shape)\n",
        "\n",
        "print(batch[0].shape)\n",
        "print(batch[1])\n",
        "print(dataset.annotation_vocab)\n",
        "print(dataset.Length())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z72ilbs5GKa1"
      },
      "source": [
        "# What was stated above still holds and i do not understand this cell :'(\n",
        "stats = [batch[0].mean((0,1,2)), batch[0].std((0,1,2))]\n",
        "print(stats)\n",
        "norm = Normalize(stats)\n",
        "def tfm_x(x): return norm(x).permute((0,3,1,2))\n",
        "t = tfm_x(batch[0])\n",
        "print([t.mean((0,2,3)),t.std((0,2,3))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7y-48OtGmQk"
      },
      "source": [
        "## Neural Nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj-Bn_gPGwWx"
      },
      "source": [
        "### Creating Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H8TSviCGyjN"
      },
      "source": [
        "# Theese next code cells are not mine so i do not understand them fully :'(\n",
        "class LayerFunction():\n",
        "  def __call__(self, *args):\n",
        "    self.args = args\n",
        "    self.out = self.forward(*args)\n",
        "    return self.out\n",
        "\n",
        "  def forward(self):\n",
        "    raise Exception('not implemented')\n",
        "  def bwd(self):\n",
        "    raise Exception('not implemented')\n",
        "  def backward(self):\n",
        "    self.bwd(self.out, *self.args)\n",
        "\n",
        "class Relu(LayerFunction):\n",
        "    def forward(self, inp): return inp.clamp_min(0.)\n",
        "    def bwd(self, out, inp): inp.g = (inp>0).float() * out.g\n",
        "\n",
        "class Lin(LayerFunction):\n",
        "    def __init__(self, w, b): self.w,self.b = w,b\n",
        "        \n",
        "    def forward(self, inp): return inp@self.w + self.b\n",
        "    \n",
        "    def bwd(self, out, inp):\n",
        "        inp.g = out.g @ self.w.t()\n",
        "        self.w.g = inp.t() @ self.out.g\n",
        "        self.b.g = out.g.sum(0)\n",
        "\n",
        "class Mse(LayerFunction):\n",
        "    def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()\n",
        "    def bwd(self, out, inp, targ): \n",
        "        inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]\n",
        "\n",
        "class Model():\n",
        "  def __init__(self, w1, b1, w2, b2):\n",
        "    self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]\n",
        "    self.loss = Mse()\n",
        "      \n",
        "  def __call__(self, x, targ):\n",
        "    for l in self.layers:\n",
        "      x = l(x)\n",
        "    return self.loss(x, targ)\n",
        "  \n",
        "  def backward(self):\n",
        "    self.loss.backward()\n",
        "    for l in reversed(self.layers):\n",
        "      l.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXUZxp6wHC2M"
      },
      "source": [
        "class MyRelu(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, i):\n",
        "        result = i.clamp_min(0.)\n",
        "        ctx.save_for_backward(i)\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        i, = ctx.saved_tensors\n",
        "        return grad_output * (i>0).float()\n",
        "\n",
        "class LinearLayer(torch.nn.Module):\n",
        "    def __init__(self, n_in, n_out):\n",
        "        super().__init__()\n",
        "        self.weight = torch.nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))\n",
        "        self.bias = torch.nn.Parameter(torch.zeros(n_out))\n",
        "    \n",
        "    def forward(self, x): return x @ self.weight.t() + self.bias\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_in,nh), torch.nn.ReLU(), torch.nn.Linear(nh,n_out))\n",
        "        self.loss = mse\n",
        "        \n",
        "    def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "215lzCJXHGgw"
      },
      "source": [
        "class Parameter(Tensor):\n",
        "    def __new__(self, x): return Tensor._make_subclass(Parameter, x, True)\n",
        "    def __init__(self, *args, **kwargs): self.requires_grad_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ9Wfa-PHJqK"
      },
      "source": [
        "class Module:\n",
        "    def __init__(self):\n",
        "        self.hook,self.params,self.children,self._training = None,[],[],False\n",
        "        \n",
        "    def register_parameters(self, *ps): self.params += ps\n",
        "    def register_modules   (self, *ms): self.children += ms\n",
        "        \n",
        "    @property\n",
        "    def training(self): return self._training\n",
        "    @training.setter\n",
        "    def training(self,v):\n",
        "        self._training = v\n",
        "        for m in self.children: m.training=v\n",
        "            \n",
        "    def parameters(self):\n",
        "        return self.params + sum([m.parameters() for m in self.children], [])\n",
        "\n",
        "    def __setattr__(self,k,v):\n",
        "        super().__setattr__(k,v)\n",
        "        if isinstance(v,Parameter): self.register_parameters(v)\n",
        "        if isinstance(v,Module):    self.register_modules(v)\n",
        "        \n",
        "    def __call__(self, *args, **kwargs):\n",
        "        res = self.forward(*args, **kwargs)\n",
        "        if self.hook is not None: self.hook(res, args)\n",
        "        return res\n",
        "    \n",
        "    def cuda(self):\n",
        "        for p in self.parameters(): p.data = p.data.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J99bioHZHYB8"
      },
      "source": [
        "class ConvLayer(Module):\n",
        "    def __init__(self, ni, nf, stride=1, bias=True, act=True):\n",
        "        super().__init__()\n",
        "        self.w = Parameter(torch.zeros(nf,ni,3,3))\n",
        "        self.b = Parameter(torch.zeros(nf)) if bias else None\n",
        "        self.act,self.stride = act,stride\n",
        "        init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_\n",
        "        init(self.w)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1)\n",
        "        if self.act: x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "class Linear(Module):\n",
        "    def __init__(self, ni, nf):\n",
        "        super().__init__()\n",
        "        self.w = Parameter(torch.zeros(nf,ni))\n",
        "        self.b = Parameter(torch.zeros(nf))\n",
        "        nn.init.xavier_normal_(self.w)\n",
        "    \n",
        "    def forward(self, x): return x@self.w.t() + self.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB2BR-8NHY0m"
      },
      "source": [
        "class T(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.c,self.l = ConvLayer(3,4),Linear(4,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVpYqaeuHZjK"
      },
      "source": [
        "class Sequential(Module):\n",
        "    def __init__(self, *layers):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.register_modules(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcREJddbHaeE"
      },
      "source": [
        "class AdaptivePool(Module):\n",
        "    def forward(self, x): return x.mean((2,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-XELc56IKJl"
      },
      "source": [
        "def nll(input, target): return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "def logsumexp(x):\n",
        "    m = x.max(-1)[0]\n",
        "    return m + (x-m[:,None]).exp().sum(-1).log()\n",
        "\n",
        "def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)\n",
        "\n",
        "def cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W-CkR5TIXDW"
      },
      "source": [
        "class Learner:\n",
        "    def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD):\n",
        "        store_attr()\n",
        "        for cb in cbs: cb.learner = self\n",
        "\n",
        "    def one_batch(self):\n",
        "        self('before_batch')\n",
        "        xb,yb = self.batch\n",
        "        self.preds = self.model(xb)\n",
        "        self.loss = self.loss_func(self.preds, yb)\n",
        "        if self.model.training:\n",
        "            self.loss.backward()\n",
        "            self.opt.step()\n",
        "        self('after_batch')\n",
        "\n",
        "    def one_epoch(self, train):\n",
        "        self.model.training = train\n",
        "        self('before_epoch')\n",
        "        dl = self.dls.train if train else self.dls.valid\n",
        "        for self.num,self.batch in enumerate(progress_bar(dl, leave=False)):\n",
        "            self.one_batch()\n",
        "        self('after_epoch')\n",
        "    \n",
        "    def fit(self, n_epochs):\n",
        "        self('before_fit')\n",
        "        self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
        "        self.n_epochs = n_epochs\n",
        "        try:\n",
        "            for self.epoch in range(n_epochs):\n",
        "                self.one_epoch(True)\n",
        "                self.one_epoch(False)\n",
        "        except CancelFitException: pass\n",
        "        self('after_fit')\n",
        "        \n",
        "    def __call__(self,name):\n",
        "        for cb in self.cbs: getattr(cb,name,noop)()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeZT5CC-IYCK"
      },
      "source": [
        "class Callback(GetAttr): _default='learner'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iBHt6spIY4O"
      },
      "source": [
        "class SetupLearnerCB(Callback):\n",
        "    def before_batch(self):\n",
        "        xb,yb = to_device(self.batch)\n",
        "        self.learner.batch = tfm_x(xb),yb\n",
        "\n",
        "    def before_fit(self): self.model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7r52SiMIcT-"
      },
      "source": [
        "class TrackResults(Callback):\n",
        "    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]\n",
        "        \n",
        "    def after_epoch(self):\n",
        "        n = sum(self.ns)\n",
        "        print(self.epoch, self.model.training,\n",
        "              sum(self.losses).item()/n, sum(self.accs).item()/n)\n",
        "        \n",
        "    def after_batch(self):\n",
        "        xb,yb = self.batch\n",
        "        acc = (self.preds.argmax(dim=1)==yb).float().sum()\n",
        "        self.accs.append(acc)\n",
        "        n = len(xb)\n",
        "        self.losses.append(self.loss*n)\n",
        "        self.ns.append(n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AacV4Ns8Ihoq"
      },
      "source": [
        "class OneCycle(Callback):\n",
        "    def __init__(self, base_lr): self.base_lr = base_lr\n",
        "    def before_fit(self): self.lrs = []\n",
        "\n",
        "    def before_batch(self):\n",
        "        if not self.model.training: return\n",
        "        n = len(self.dls.train)\n",
        "        bn = self.epoch*n + self.num\n",
        "        mn = self.n_epochs*n\n",
        "        pct = bn/mn\n",
        "        pct_start,div_start = 0.25,10\n",
        "        if pct<pct_start:\n",
        "            pct /= pct_start\n",
        "            lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr\n",
        "        else:\n",
        "            pct = (pct-pct_start)/(1-pct_start)\n",
        "            lr = (1-pct)*self.base_lr\n",
        "        self.opt.lr = lr\n",
        "        self.lrs.append(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jicc8HQyHNa9"
      },
      "source": [
        "### Using created Neural Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QlwMcdcHSf4"
      },
      "source": [
        "# Theese next code cells are not mine so i do not understand them fully :'(\n",
        "x = torch.randn(200, 100)\n",
        "y = torch.randn(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPCP89MhHTgX"
      },
      "source": [
        "w1 = torch.randn(100,50) * sqrt(2 / 100)\n",
        "b1 = torch.zeros(50)\n",
        "w2 = torch.randn(50,1) * sqrt(2 / 50)\n",
        "b2 = torch.zeros(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd0n-dI5HUV9"
      },
      "source": [
        "Parameter(torch.zeros(50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpTQPY-dHco8"
      },
      "source": [
        "def simple_cnn():\n",
        "    return Sequential(\n",
        "        ConvLayer(3 ,16 ,stride=2), #32\n",
        "        ConvLayer(16,32 ,stride=2), #16\n",
        "        ConvLayer(32,64 ,stride=2), # 8\n",
        "        ConvLayer(64,128,stride=2), # 4\n",
        "        AdaptivePool(),\n",
        "        Linear(128, 10)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJZhfh0pHdt1"
      },
      "source": [
        "m = simple_cnn()\n",
        "len(m.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNGY1Rb4JApv"
      },
      "source": [
        "def print_stats(outp, inp): print (outp.mean().item(),outp.std().item())\n",
        "for i in range(4): m.layers[i].hook = print_stats\n",
        "xbt = tfm_x(batch[0])\n",
        "r = m(xbt)\n",
        "r.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdmzrcEOJEpu"
      },
      "source": [
        "loss = nll(sm, batch[1])\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGkCeIyoJFBK"
      },
      "source": [
        "sm = log_softmax(r); sm[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD0FHEjnJFHK"
      },
      "source": [
        "x = torch.rand(5)\n",
        "a = x.max()\n",
        "x.exp().sum().log() == a + (x-a).exp().sum().log()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB3O8yMoJJeU"
      },
      "source": [
        "logsumexp(r)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv1vvtveJK_w"
      },
      "source": [
        "sm = log_softmax(r); sm[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIDtu--LHe4J"
      },
      "source": [
        "cbs = [SetupLearnerCB(),TrackResults()]\n",
        "learn = Learner(simple_cnn(), data_loaders, cross_entropy, lr=0.1, cbs=cbs)\n",
        "learn.fit(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-Axj8gfIkMc"
      },
      "source": [
        "onecyc = OneCycle(0.1)\n",
        "learn = Learner(simple_cnn(), data_loaders, cross_entropy, lr=0.1, cbs=cbs+[onecyc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySY3qU8wIlRB"
      },
      "source": [
        "learn.fit(8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1zBVEiGImPL"
      },
      "source": [
        "plt.plot(onecyc.lrs);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLVEX-QRZ6qH"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJaK-pAmZ-Ml"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}